---
title: "Predicting Ocean Health"
subtitle: "Natural Language Processing with BBNJ Speech Data"
author: "Shruti Rathnavel"
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
    code_folding: "show"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(wordcloud)
```

## Introduction

```{r}
load("data/bbnj_dat.rda")
```

The dataset is speeches from the UN's Intergovernmental Conference on Marine Biodiversity in Areas Beyond National Jurisdiction. The goal of the conference is to create an international and binding agreement under the UN Convention of the Law of the Sea (UNCLOS) to govern the conversation and sustainable use of biodiversity beyond national jurisdictions (BBNJ). 

I scraped over 200 speeches across 5 substantive sessions of the conference (scraping code can be found in `scraping.R`). I use the data in the speeches to predict the country's score on the Ocean Health Index, which aggregates information across the following goal areas to measure how countries treat the marine areas within their jurisdictions. 

```{r}
goals
```
<br>

## EDA

I performed an exploratory data analysis on the whole training set.

```{r, warning=FALSE, message=FALSE}
load("eda-vis.rda")
```

### Index

I mapped the OHI values across the whole world map. 

```{r, warning=FALSE, message=FALSE}
index_map
```

Then I limited it to just the countries whose speeches are represented in my cleaned training dataset. With around 100 observations, and only 40 distinct countries, this model is not likely to be very useful in practice. 

```{r, warning=FALSE, message=FALSE}
bbnj_countries_map
```

### Wordclouds by country 

I split the speech data into quartiles by index, so that I can compare the most frequently occurring words across low-OHI countries (the first cloud - bottom 25 %ile) and high-OHI (the second cloud - top 25 %ile) countries.  

```{r, warning=FALSE, message=FALSE}
cloud1 %>%
  with(wordcloud(words, sum, max.words = 50, rot.per = 0))
```
```{r, warning=FALSE, message=FALSE}
cloud4 %>%
  with(wordcloud(words, sum, max.words = 50, rot.per = 0))
```


## Modelling

### Pre-processing

I parsed and lemmatized the data with the spacy package (can be seen in `text_processing.R`). I used the following recipe for all my models - I ran it with max_tokens = 50 before and got a slightly better RMSE for my troubles when I increased it. 

I thought ngrams would be better than words in this case because lots of the topics being discussed as part of the BBNJ framework are multiple words long (marine genetic resources, area based management tools). I used a tf-idf step, which compares term frequency in a speech to inverse document frequency across all the speeches, effectively identifying word uniqueness. 

```{r, eval=FALSE, message=FALSE, warning=FALSE}
bbnj_rec <- recipe(index ~ lemmas, data = bbnj_train) %>%
  step_tokenize(lemmas, token = "ngrams") %>% 
  step_tokenfilter(lemmas, min_times = 10, max_tokens = 100) %>% 
  step_tfidf(lemmas)
```


### Candidate models

```{r}
load("results/kknn_tuned.rda")
load("results/rf_tuned.rda")
load("results/linear_tuned.rda")
```

**1. Linear regression**

I ran glmnet models across a tuning grid for penalty and mixture. 

```{r}
tune::autoplot(linear_tuned)
```


**2. Random forest**

I ran random forest models across a tuning grid for mtry and min_n. The best rmse of all my candidate models was the random forest with min_n = 2 and mtry = 1, so this is what I fitted to the test set.

```{r}
tune::autoplot(rf_tuned)
```

**3. K-nearest neighbors**

I ran knn across a tuning grid for neighbors. 

```{r}
tune::autoplot(kknn_tuned)
```

### Fit to test set

```{r}
load("results/performance.rda")
load("results/plots.rda")
```


## Conclusions

## Citations

